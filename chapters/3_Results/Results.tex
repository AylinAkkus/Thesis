\chapter{Results}
\label{ch:results}
\section{Benchmarking Performance}
\label{sec:results_gelato30b}
To evaluate the overall effectiveness of the Gelato training recipe, we compare it against the leading open-source model, GTA1 \cite{yang2025gta1}. We initialize RL training from UI-TARS-1.5-7B \cite{qin2025uitars} (as done for GTA1-7B-2507) and train on Click-100k until convergence (255 steps). Gelato outperforms GTA1-7B-2507 on both offline benchmarks ScreenSpot-Pro and OS-World-G, with particularly large improvements on OS-World-G.


Figure~\ref{fig:final_performance} places Gelato-30B-A3B in context against leading models. Despite using only 3B active parameters per token, Gelato-30B-A3B achieves \textbf{63.8\%} on ScreenSpot-Pro and \textbf{69.1\%} on OS-World-G, outperforming GTA1-32B, Qwen3-VL-235B-Instruct, and OpenCUA-72B on both benchmarks. Detailed per-category results are presented in Section~\ref{sec:results_gelato30b}.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/FinalPerformance.png}
\caption{Final benchmark comparison of Gelato-30B-A3B against leading models. \textbf{Left:} OS-World-G accuracy. \textbf{Right:} ScreenSpot-Pro accuracy. Gelato-30B-A3B achieves the highest accuracy on both benchmarks despite having fewer active parameters than all competitors.}
\label{fig:final_performance}
\end{figure}

\paragraph{Performance with Refusal}
We elicit refusal behavior, the ability to decline grounding when the target element cannot be located, from Gelato-30B-A3B without explicitly training for it. By appending ``If you cannot find the element, return refusal'' to the instruction prompt and including refusal cases in the evaluation (previously treated as zero-accuracy), we raise overall accuracy on OS-World-G to 69.15\% (+1.96~pp) and on OS-World-G (Refined) to 74.65\% (+1.25~pp).

\section{OS-World Agent}

To measure end-to-end agent performance, we evaluate Gelato-30B-A3B on the OS-World benchmark as the grounding module together with GPT-5.

\subsection{Agent Harness}
We evaluate Gelato-30B-A3B as a computer-use agent on OS-World using the GTA1.5 agent framework. \cite{osworld_gta15_agent}. The agent uses GPT-5 as a planning model and has a maximum of 50 steps, waiting 3 seconds between actions. We made minor modifications to the agent code, including a change to properly invoke the spreadsheet cell modification tool and an additional delay between trajectory completion and evaluation to ensure the VM state fully updates.

\subsection{Reproducibility Issues}
We found that many of the issues discussed in the EpochAI article critiquing OS-World \cite{brand2025osworld} significantly affect reproducibility. Benchmarking agent performance proved challenging due to:
\begin{enumerate}
    \item Non-deterministic planner behavior combined with insufficient trial repetitions, making fair comparison against prior work difficult.
    \item Changing evaluation prompts without explicit versioning.
    \item Incomplete evaluation coverage and ambiguous task specifications that fail to recognize valid alternative solutions.
\end{enumerate}

To enable fair comparison, we ran three trials for both Gelato-30B-A3B and GTA1-32B in the same agent harness. Gelato-30B-A3B achieves \textbf{58.71 $\pm$ 0.66\%} success rate on OS-World automated evaluation, performing on par or above GTA1-32B (\textbf{56.97 $\pm$ 1.47\%} success rate).

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/gelato-fig7.png}
\caption{OS-World agent performance across three runs with GPT-5 planner. Automated evaluation underestimates performance due to incomplete task specifications. Human evaluation shows Gelato-30B-A3B achieves \textbf{61.85\%} success rate vs.\ GTA1-32B's \textbf{59.47\%}.}
\label{fig:osworld_agent}
\end{figure}

\subsection{Human Evaluation}

We manually identified 20 tasks where the evaluation function is incomprehensive or the task specification is ambiguous. For each task, we review all agent trajectories and determine whether the task was successfully completed (Figure~\ref{fig:osworld_agent}). With human evaluation corrections, Gelato-30B-A3B achieves \textbf{61.85 $\pm$ 0.79\%} success rate compared to the automated evaluation result of \textbf{58.71 $\pm$ 0.66\%}. Similarly, GTA1-32B achieves \textbf{59.47 $\pm$ 1.27\%} success rate with human evaluation compared to \textbf{56.97 $\pm$ 1.47\%} on automated evaluation.
