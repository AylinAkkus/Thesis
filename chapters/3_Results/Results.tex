\chapter{Results}
\label{ch:results}

In this chapter, we present comprehensive evaluation results for Gelato-30B-A3B. We evaluate on isolated grounding benchmarks (ScreenSpot-Pro and OS-World-G) to measure grounding accuracy, and on the full OS-World agent benchmark to assess end-to-end agent performance. We compare against prior state-of-the-art models throughout.

\section{Grounding Benchmark Evaluation}
\label{sec:results_gelato30b}

\paragraph{Gelato vs.\ GTA1 Recipe.}
We first compare the Gelato training recipe to the GTA1 training recipe, since GTA1 is the prior state-of-the-art model for computer-use grounding. We initialize RL training from UI-TARS-1.5-7B and train on Click-100k until convergence (255 steps) using DAPO. As shown in Figure~\ref{fig:recipe_comparison}, Gelato outperforms GTA1-7B-2507 on both ScreenSpot-Pro and OS-World-G, with particularly large improvements on OS-World-G.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/FinalRecipeComparison.png}
    \caption{Comparison of training recipes applied to the same UI-TARS-1.5-7B base model. The Gelato recipe (DAPO on Click-100k) outperforms the GTA recipe on both ScreenSpot-Pro (+8.6~pp over base, +0.7~pp over GTA) and OS-World-G (+6.2~pp over base, +3.7~pp over GTA).}
    \label{fig:recipe_comparison}
\end{figure}

\medskip
\paragraph{Gelato vs.\ Other Models.}
Figure~\ref{fig:final_performance} places Gelato-30B-A3B in context against leading models, including the specialized grounding model GTA1 as well as end-to-end computer-use models (Qwen3-VL-30B-A3B-Instruct, Qwen2.5-VL-235B-A22B-Instruct, OpenCUA-72B). Despite using only 3B active parameters per token, Gelato-30B-A3B achieves \textbf{63.8\%} on ScreenSpot-Pro and \textbf{69.1\%} on OS-World-G, outperforming all other models on both benchmarks.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/FinalPerformance.png}
\caption{Final benchmark comparison of Gelato-30B-A3B against leading models. \textbf{Left:} OS-World-G accuracy. \textbf{Right:} ScreenSpot-Pro accuracy. Gelato-30B-A3B achieves the highest accuracy on both benchmarks despite having fewer active parameters than all competitors.}
\label{fig:final_performance}
\end{figure}

\paragraph{Performance with Refusal.}
The OS-World-G benchmark includes a refusal subset where the correct answer is to decline grounding the instruction. We elicit refusal behavior from Gelato-30B-A3B without explicitly training for it. By appending ``If you cannot find the element, return refusal'' to the instruction prompt and including refusal cases in the evaluation (previously treated as zero-accuracy), we raise overall accuracy on OS-World-G to \textbf{69.15\%} (+1.96~pp) and on OS-World-G (Refined) to \textbf{74.65\%} (+1.25~pp).

\section{OS-World Agent Evaluation}

Having evaluated on isolated grounding benchmarks, we now assess Gelato-30B-A3B on the full OS-World agent benchmark, which measures end-to-end computer-use performance.

\subsection{Agent Harness}

Since Gelato-30B-A3B follows the two-stage paradigm (Section~\ref{par:end_to_end_vs_two_stage}), we use the GTA1.5 agent framework~\cite{osworld_gta15_agent} to evaluate it. The agent architecture combines:
\begin{itemize}
    \item \textbf{Planning model}: GPT-5 generates high-level action plans
    \item \textbf{Grounding model}: Gelato-30B-A3B grounds instructions to specific UI locations
    \item \textbf{Action execution}: The system executes clicks, keyboard input, and other actions
\end{itemize}
The agent has a maximum of 50 steps per task and waits 3 seconds between actions to allow the interface to update. We made minor modifications to the agent code, including a fix to the spreadsheet cell modification tool invocation and an additional delay between trajectory completion and evaluation to ensure the VM state fully updates.

\paragraph{Reproducibility Issues.}
We found that many of the issues discussed in the EpochAI article critiquing OS-World~\cite{brand2025osworld} significantly affect reproducibility. Benchmarking agent performance proved challenging due to:
\begin{enumerate}
    \item Non-deterministic planner behavior combined with insufficient trial repetitions, making fair comparison against prior work difficult.
    \item Changing evaluation prompts by OS-World authors without explicit versioning.
    \item Incomplete evaluation coverage and ambiguous task specifications that fail to recognize valid alternative solutions.
\end{enumerate}

\paragraph{Automated Evaluation Results.}
To enable fair comparison, we conducted three independent trials for both Gelato-30B-A3B and GTA1-32B in the same agent harness. All experiments were conducted on a fixed snapshot of OS-World to ensure reproducibility. Gelato-30B-A3B achieves \textbf{58.71 $\pm$ 0.66\%} success rate on OS-World automated evaluation, performing on par with or above GTA1-32B (\textbf{56.97 $\pm$ 1.47\%} success rate), thereby surpassing the previous state-of-the-art.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/gelato-fig7.png}
\caption{OS-World agent performance across three runs with GPT-5 planner. Automated evaluation underestimates performance due to incomplete task specifications. Human evaluation shows Gelato-30B-A3B achieves \textbf{61.85\%} success rate vs.\ GTA1-32B's \textbf{59.47\%}.}
\label{fig:osworld_agent}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
    \hline
    \textbf{Model} & \textbf{Params} & \textbf{Architecture} & \textbf{OS-World (\%)} \\
    \hline
    Qwen2.5-VL-32B-Instruct & 32B & E2E & 8.83 \\
    UI-TARS-7B & 7B & E2E & 24.6 \\
    Qwen3-VL-30B-A3B-Instruct & 30B & E2E & 38.1 \\
    UI-TARS-1.5-7B & 7B & E2E & 42.5 \\
    OpenCUA-72B & 72B & E2E & 45.0 \\
    UI-TARS-2 & -- & E2E & 47.5 \\
    GTA1-32B + GPT-5 & 32B & 2S & 56.97 $\pm$ 1.47 \\
    \textbf{Gelato-30B-A3B + GPT-5} & \textbf{30B} & \textbf{2S} & \textbf{58.71 $\pm$ 0.66} \\
    \hline
    \end{tabular}
    \caption{Open-weights agent performance on OS-World (automated evaluation). \emph{Architecture}: End-to-End (E2E) models handle both planning and grounding; Two-Stage (2S) models use a separate planner (GPT-5) and grounding module.}
    \label{tab:osworld_agents}
\end{table}

\section{Human Evaluation}

Automated evaluation metrics can underestimate agent performance due to incomplete task specifications and ambiguous evaluation criteria. To obtain a more accurate estimate, we conduct human evaluation on a subset of tasks.

\subsection{Methodology}

We manually identified 20 tasks where the automated evaluation function is incomplete or the task specification is ambiguous. For each task, we review all agent trajectories across all runs and determine whether the task was successfully completed according to the intended goal.

Common issues with automated evaluation include:
\begin{itemize}
    \item Evaluation functions that check only one valid solution path when multiple valid approaches exist
    \item Timing issues where the evaluation runs before the final state is saved
    \item Over-specific checks that reject valid alternative solutions
\end{itemize}

\subsection{Human Evaluation Results}

With human evaluation corrections (Figure~\ref{fig:osworld_agent}), Gelato-30B-A3B achieves \textbf{61.85 $\pm$ 0.79\%} success rate compared to the automated result of \textbf{58.71 $\pm$ 0.66\%}. Similarly, GTA1-32B achieves \textbf{59.47 $\pm$ 1.27\%} with human evaluation compared to \textbf{56.97 $\pm$ 1.47\%} on automated evaluation. This amounts to a +3.14~pp gain for Gelato-30B-A3B and a +2.50~pp gain for GTA1-32B, confirming that automated evaluation significantly underestimates true agent performance. Importantly, the relative ranking between models remains consistent: Gelato-30B-A3B maintains its advantage over GTA1-32B under human evaluation.

\medskip
Note that this is not a comprehensive manual evaluation---we focused on clearly problematic tasks that were straightforward to verify. A more extensive human evaluation could reveal additional cases where automated metrics underestimate performance.

\section{Evaluation Summary}

Our evaluation demonstrates that Gelato-30B-A3B achieves state-of-the-art performance across multiple benchmarks:

\begin{enumerate}
    \item \textbf{Isolated grounding}: Surpasses specialized grounding models and much larger VLMs on both ScreenSpot-Pro and OS-World-G.
    \item \textbf{Superior data and training recipe}: The performance gains can be attributed to our curated Click-100k dataset and DAPO-based training recipe, as demonstrated by the controlled comparison against GTA1 on the same base model.
    \item \textbf{End-to-end agent performance}: Outperforms GTA1-32B and other end-to-end computer-use models on OS-World under both automated and human evaluation.
\end{enumerate}

These results validate our approach to data curation, filtering, and RL training, demonstrating that careful attention to data quality and training methodology yields significant improvements in grounding model capabilities.
