\section{Related Work}

\paragraph{Closed-Source Computer-Use Models}
Several commercial systems incorporate GUI grounding and computer-use capabilities, including GPT-4o with vision, Operator by OpenAI, Claude 4.5 Sonnet by Anthropic, and Gemini 3 Pro by Google. These systems remain entirely closed-source: training procedures, datasets, model architectures, and implementation details are undisclosed, precluding detailed comparison or reproducibility.

\paragraph{General-Purpose Vision-Language Models}
Recent open-weights vision-language models such as Qwen2.5-VL and Qwen3-VL \cite{qwen2025qwen25vl, qwen2025qwen3vl} incorporate domain-specific data targeting GUI interaction and computer-use scenarios alongside standard multimodal pretraining corpora. In addition to image-text, OCR, and general grounding data, these models include screenshot-based supervision, multi-step agent trajectories, and function-calling data, with evaluations reported on agentic and GUI-centric benchmarks. Despite this increasing focus on computer-use capabilities, these models remain general-purpose multimodal foundation models rather than task-specialized agents, and the precise training configuration for domain-specific components is described only at a high level.

\paragraph{End-to-End GUI Agents}
Building on Qwen-based vision-language backbones, the UI-TARS series \cite{qin2025uitars, qin2025uitars15, wang2025uitars2} shifts toward explicitly training end-to-end GUI agents. UI-TARS applies continual training from Qwen2-VL 7B and 72B on approximately 50B tokens, incorporating large-scale GUI screenshot perception data, grounding annotations, action traces, and 6M filtered GUI tutorials to support System-2 reasoning and multi-step execution. UI-TARS-2 further scales this paradigm through a data flywheel combining continual pretraining, supervised fine-tuning, and multi-turn reinforcement learning in a unified GUI sandbox environment, enabling large-scale interactive rollouts across thousands of VMs. Empirically, UI-TARS-2 reports strong performance on GUI-centric benchmarks, achieving 94.2\% on ScreenSpot-V2, 61.6\% on ScreenSpot-Pro, and 47.5\% on OSWorld. While these works demonstrate substantial progress toward end-to-end computer-use agents, the large-scale trajectory data, annotation pipelines, and reinforcement learning infrastructure underlying these results remain closed-source, limiting full reproducibility and controlled comparison.

\paragraph{Specialized GUI Grounding Models}
Several recent works share our focus on specialized GUI grounding through data-centric approaches. JEDI \cite{xie2025jedi} and SeeClick \cite{cheng2024seeclick} investigate data collection and filtering methods and open-source their datasets. However, these works rely on continual pretraining through supervised fine-tuning alone, whereas our work incorporates reinforcement learning to directly optimize for grounding accuracy.

Two works are particularly closely related to our approach in both methodology and philosophy: SE-GUI \cite{yuan2025segui} and GTA1 \cite{yang2025gta1}. SE-GUI trains a GUI grounding agent using Group Relative Policy Optimization (GRPO) with a dense spatial reward that encourages predicted click points to lie inside the ground-truth bounding box. Like our work, SE-GUI emphasizes data quality and difficulty filtering, curating a high-quality dataset by removing noisy annotations and overly simple samples that the base model already solves reliably. Their data filtering strategy directly inspired aspects of our own filtering pipeline.

GTA1 \cite{yang2025gta1} presents a two-stage GUI agent that significantly advanced grounding performance. Similar to our approach, the authors adopt a reinforcement learning framework based on GRPO, directly rewarding successful clicks when predicted coordinates fall inside the ground-truth bounding box. They introduce a data-cleaning strategy that filters noisy annotations via an IoU-based consistency check using an external UI element detector, thereby improving supervision quality. This explicit attention to data quality through model-based filtering informed our own use of OmniParser for annotation validation. Both SE-GUI and GTA1 demonstrate that combining reinforcement learning with careful data curation yields substantial improvements over pure supervised fine-tuning, a finding that motivates our SFT-then-RL training approach and systematic filtering pipeline.



