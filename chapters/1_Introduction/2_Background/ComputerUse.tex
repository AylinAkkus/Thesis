\subsection{Computer Use Agents}
Computer-use agents enable Large Language Models (LLMs) to operate computers autonomously through a perceptionâ€“action loop. The agent observes the interface state, predicts an action, executes it, and receives feedback until task completion. Formally, the agent learns a policy:
\[
(\text{instruction}, \text{history}, \text{current observation}) \rightarrow \text{next action}
\]
where actions are low-level operations (clicks, typing, scrolling) that decompose high-level natural language instructions into executable GUI interactions.

\paragraph{Language-based vs. Vision-based Agents}
Early language-only approaches operated over structured textual representations (DOM trees, accessibility trees) to generate tool calls (Figure~\ref{fig:grounding_comparison}a). While effective in constrained settings, these methods require privileged system access and handcrafted filtering pipelines, limiting generalization to visually-dependent tasks. Vision-based agents using VLMs instead process raw screenshots (Figure~\ref{fig:grounding_comparison}b), eliminating dependency on internal program structures. This paradigm shift changes the observation space from structured text to high-dimensional visual input, requiring models to extract semantic meaning and identify interactive elements directly from pixels.

\paragraph{Planner-Grounder Framework}
Vision-based agents decompose interaction into two problems: deciding \emph{what} to do (planning) and \emph{where} to execute it (grounding). The \emph{planner} operates at the semantic level, reasoning over instruction, screenshot, and interaction history to propose abstract actions (e.g., "click the Settings button"). The \emph{grounder} operates at the perceptual level, mapping abstract action descriptions to precise screen coordinates. This modularization enables specialization: stronger reasoning models serve as planners while lightweight models optimize spatial localization.

\paragraph{GUI Grounding}
GUI grounding maps abstract interaction intents to spatial locations. Given screenshot $s$ and action proposal $a$, the grounding model predicts target coordinates:
\[
f_{\text{ground}}: (s, a) \rightarrow (x, y)
\]
This task is challenging due to information-dense layouts with numerous small elements, significant cross-platform interface variation, and reliance on visual cues (color, typography, proximity) for element discrimination. Early supervised approaches predicted bounding box centers, creating train-test misalignment: any coordinate within the target region constitutes valid interaction, yet center-point regression penalizes such predictions. Recent methods directly optimize for region-based success, aligning training objectives with actual interaction requirements.
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth,height=5cm,keepaspectratio]{figures/LanguageGrounding.png}
        \caption{Language-based grounding: The model receives a filtered DOM tree as input and predicts coordinates based on structured text representations of UI elements.}
        \label{fig:language_grounding}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth,height=5cm,keepaspectratio]{figures/VisualGroundingTask.png}
        \caption{Visual grounding: The model operates directly on screenshots and predicts coordinates from high-dimensional visual input.}
        \label{fig:visual_grounding}
    \end{subfigure}
    \caption{Comparison of grounding approaches. Both methods receive the instruction ``Open a new tab'' and predict coordinates, but differ in their input modality: (a) language-based grounding uses structured DOM representations, while (b) visual grounding processes raw screenshots.}
    \label{fig:grounding_comparison}
\end{figure}



