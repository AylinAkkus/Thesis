\subsection{GUI Grounding Datasets}

We collected, normalized and filtered multiple open source datasets for GUI grounding tasks.

\textbf{ShowUI}~\cite{lin2024showui} presents a vision-language-action model for GUI visual agents, providing grounding data for both web and desktop interfaces. ShowUI demonstrates the importance of unified representations across different interface types.

\textbf{AutoGUI}~\cite{li2025autogui} scales GUI grounding with automatic functional annotation, introducing methods to automatically generate grounding annotations from interface interactions.

\textbf{SeeClick}~\cite{cheng2024seeclick} harnesses GUI grounding for advanced visual GUI agents, contributing early work on connecting natural language instructions to visual elements.

\textbf{PixMo Points}~\cite{deitke2024molmo}, part of the Molmo project, provides open weights and open data for state-of-the-art vision-language models, including pointing and grounding capabilities.

\textbf{OS-Atlas}~\cite{wu2024osatlas} introduces a foundation action model for generalist GUI agents, providing grounding data across diverse operating system interfaces.

\textbf{UGround}~\cite{gou2024uground} focuses on navigating the digital world as humans do through universal visual grounding for GUI agents.

\textbf{WaveUI}~\cite{agentsea2024waveui} contributes additional web and mobile interface grounding data.

\textbf{PC-Agent-E}~\cite{he2025pcagente} demonstrates efficient agent training for computer use by extracting grounding annotations from recorded trajectories.

\textbf{UI-VISION}~\cite{nayak2025uivision} provides a desktop-centric GUI benchmark for visual perception and interaction, with particular emphasis on professional applications.

