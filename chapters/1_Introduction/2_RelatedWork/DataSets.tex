\section{GUI Grounding Datasets}

Several datasets have been developed for training GUI grounding models:

\textbf{ShowUI}~\cite{showui} presents a vision-language-action model for GUI visual agents, providing grounding data for both web and desktop interfaces. ShowUI demonstrates the importance of unified representations across different interface types.

\textbf{AutoGUI}~\cite{autogui} scales GUI grounding with automatic functional annotation, introducing methods to automatically generate grounding annotations from interface interactions.

\textbf{SeeClick}~\cite{seeclick} harnesses GUI grounding for advanced visual GUI agents, contributing early work on connecting natural language instructions to visual elements.

\textbf{PixMo Points}~\cite{pixmo}, part of the Molmo project, provides open weights and open data for state-of-the-art vision-language models, including pointing and grounding capabilities.

\textbf{OS-Atlas}~\cite{osatlas} introduces a foundation action model for generalist GUI agents, providing grounding data across diverse operating system interfaces.

\textbf{UGround}~\cite{uground} focuses on navigating the digital world as humans do through universal visual grounding for GUI agents.

\textbf{WaveUI}~\cite{waveui} contributes additional web and mobile interface grounding data.

\textbf{PC-Agent-E}~\cite{pcagente} demonstrates efficient agent training for computer use by extracting grounding annotations from recorded trajectories.

\textbf{UI-VISION}~\cite{uivision} provides a desktop-centric GUI benchmark for visual perception and interaction, with particular emphasis on professional applications.

