\section{Motivation}
Recent progress in machine learning has been largely driven by scaling laws~\cite{kaplan2020scaling,hestness2017deeplearningscalingpredictable,aghajanyan2023scalinglawsgenerativemixedmodal,cherti2023reproducible}, as demonstrated by models such as GPT-4, Stable Diffusion, and CLIP. These advances rest on three pillars: (1) compute, (2) model size, and (3) training data. Of these, training data has received the least systematic attention—despite its crucial role, datasets themselves are rarely the subject of active research~\cite{sambasivan2021dataset}. On the model side, experimentation is relatively straightforward: given sufficient compute, permutations of width, depth, normalization, and training hyperparameters can be rigorously evaluated, yielding consistent improvements over the years (Touvron et al., 2023a,b; Elsen et al., 2023). The dataset side, however, is murkier. Most large-scale training sets are not publicly released, leaving the community to attempt open reproductions (Schuhmann et al., 2021, 2022; Byeon et al., 2022; Gao et al., 2020)—efforts that are often one-off and lack the iterative refinement that models enjoy. While recent initiatives such as DataPerf, DataComp, and MetaCLIP (Mazumder et al., 2022; Gadre et al., 2023; Xu et al., 2023) have begun to bridge this gap by providing consistent evaluation and reproduction frameworks, we argue that dataset design can and should leverage the same principled methodology as model design. In particular, large-scale dataset construction can generally be decomposed into two phases: uncurated data collection and dataset filtering.

This challenge is especially pronounced in the emerging domain of Computer-Use agents. Graphical User Interfaces (GUIs) are central to how people engage with the digital world, serving as the primary medium for a wide range of daily activities. Large Language Models (LLMs) [32], originally developed as conversational chatbots, have proven capable of much more: their ability to comprehend complex language instructions and seamlessly integrate tools has revealed significant potential for automating complex tasks through Computer-Use agents [1, 13, 16, 56]. This progress has inspired the development of intelligent agents that can substantially streamline human workflows based on natural language instructions.

Early work in this area focused on language-only agents [12, 47, 55] that rely on closed-source, API-based LLMs such as GPT-4 [32], leveraging text-rich metadata like HTML inputs or accessibility trees to perform navigation and other tasks. However, this text-only paradigm faces fundamental limitations in practice: (1) unlike web environments, general Computer-Use scenarios do not always provide rich API-based metadata, and (2) users typically interact with interfaces visually—through screenshots—without access to the underlying structural information. These limitations underscore the need for Computer-Use visual agents that can perceive and interact with UIs in the same way humans do.

Given a user instruction, the Computer-Use task can be broadly decomposed into two components: (1) \textit{planning}—generating one or a sequence of actions to execute—and (2) \textit{grounding}—mapping the relevant UI element(s) to precise coordinates on the screen by autoregressively predicting the bounding box or - in our case - the center of the target UI element. Figure~\ref{fig:grounding_task} illustrates this grounding process: given a screenshot and a natural language instruction, a vision-language model predicts the target coordinates on the screen. This decomposition highlights the core difficulty of the problem: it demands both rich semantic understanding of UI elements and accurate pixel-level localization. To address these requirements, researchers have begun training vision-language models with these capabilities in mind. For instance, studies such as [11,15,17] utilize web screenshot datasets to enhance large multi-modal models' element-grounding abilities.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/GroundingTask.png}
\caption{Overview of the grounding task. Given a screenshot and a user instruction (e.g., ``Open a new tab.''), a Vision-Language Model (VLM) predicts the coordinates of the target UI element on the screen.}
\label{fig:grounding_task}
\end{figure}

Despite this progress, training multi-modal models for GUI visual agents continues to face several significant challenges:
\begin{enumerate}[label=(\alph*)]
    \item \textbf{Availability of Training Data:} Many state-of-the-art models remain closed-source, restricting access to their training data. Even among open-source models, the training data or its exact specification is often unavailable, making it difficult to reproduce or build upon existing work.
    \item \textbf{Training Data Composition:} Similar to the challenges faced by language-based Computer-Use agents, the availability of rich APIs for the web has led to an overrepresentation of web data in existing training sets. Professional application data, by contrast, is difficult to obtain with reliable annotations.
    \item \textbf{Resolution and Aspect Ratio Variability:} If Computer-Use agents are to be deployed in the real world rather than on standardized virtual machines, they must contend with widely varying screenshot aspect ratios, resolutions, and sizes. This necessitates either resizing screenshots—at the cost of information loss—or training models that can generalize across different resolutions and display configurations.
\end{enumerate}
