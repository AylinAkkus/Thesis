\chapter{Appendix}

\section{Data Source Examples}
\label{sec:appendix_data_sources}

Figure~\ref{fig:data_source_samples} shows representative examples from six of the data sources that comprise the Click-100k training set. Each sample consists of a screenshot paired with a natural language grounding instruction. The sources span web pages (ShowUI-Web, WaveUI), desktop applications across multiple operating systems (PC-Agent-E, OmniAct, ShowUI-Desktop), and mixed environments (OS-Atlas), illustrating the diversity of GUI contexts in the data pool.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/data_source_samples.png}
\caption{Example training samples from six data sources in Click-100k. Each panel shows a screenshot with its associated grounding instruction (italic text below). The data pool covers web browsing, desktop applications, and professional software across Windows, macOS, and Linux environments.}
\label{fig:data_source_samples}
\end{figure}

\section{YouTube Tutorial Application List}

This section lists the professional applications covered by the YouTube tutorial data collection pipeline described in Section~\ref{sec:video_data_collection}.

\begin{table}[h]
\centering
\caption{Complete list of professional applications covered by the YouTube tutorial data collection pipeline.}
\label{tab:youtube_apps}
\begin{tabular}{llll}
\hline
3ds Max & Adobe Acrobat & Adobe After Effects & Adobe Dreamweaver \\
Adobe Illustrator & Adobe InDesign & Adobe Lightroom & Adobe Photoshop \\
Adobe Premiere & Airmail & Altium Designer & Android Studio \\
ANSYS & Apple Mail & Asana & Atom \\
AutoCAD & Autodesk Eagle & Autodesk Inventor & Autodesk Maya \\
Autodesk Revit & Avid Media Composer & Axure & Abaqus \\
Balsamiq & Blender & Brave & Burp Suite \\
Cadence Virtuoso & Catia & Cinema4D & COMSOL \\
Confluence & CryEngine & Cubase & DaVinci Resolve \\
DBeaver & Eviews & Figma & Final Cut Pro \\
FL Studio & Framer & Fusion 360 & GameMaker \\
GIMP & IBM SPSS & Intel Quartus Prime & IntelliJ IDEA \\
Jupyter & KiCad & LabVIEW & LibreOffice Base \\
LibreOffice Calc & LibreOffice Draw & LibreOffice Impress & LibreOffice Math \\
LibreOffice Writer & Logic Pro X & Looker & Mailbird \\
MATLAB & Microsoft Edge & ModelSim & Mozilla Firefox \\
Notion & OBS & Obsidian & OneNote \\
OriginLab & Outlook & Power BI & PyCharm \\
Quartus & RStudio & Simulink & SolidWorks \\
Stata & Tableau & Thunderbird & Unity \\
Unreal Engine & VLC Media Player & VMWare & Vivado \\
Xcode & & & \\
\hline
\end{tabular}
\end{table}

\section{Claude Prompt for Video Frame Annotation}
\label{sec:appendix_claude_video_prompt}

The following prompt was used with Claude to generate structured grounding instructions from professional application screenshots collected via the video data pipeline (Section~\ref{sec:video_data_collection}).

\begin{figure}[p]
\vspace*{-2em}
\begin{lstlisting}[
  basicstyle=\ttfamily\tiny,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{black!5},
  xleftmargin=0.5em,
  xrightmargin=0.5em,
  aboveskip=0.3em,
  belowskip=0.3em,
  abovecaptionskip=0.5em,
  belowcaptionskip=0pt,
  caption={Prompt template used for Claude-based annotation of professional application screenshots.},
  label={lst:claude_video_prompt}
]
Analyze this user interface image and provide helpful
instructions for using this application.

First, identify the main segments/regions of the interface
(e.g., "top toolbar", "left sidebar", "main canvas",
"color palette popup", "file menu dropdown", etc.).

Then, generate 20-30 diverse, specific instructions
organized by these segments. Focus on:

1. Common tasks a user might want to accomplish
2. How to navigate and use the visible features
3. Workflow suggestions and tips
4. Specific actions related to the tools and functions shown

CRITICAL INSTRUCTION REQUIREMENTS:
- All instructions must be written in English (US), even
  if the interface uses another language
- ALL instructions must be LEFT CLICK based actions only
  (no right click, typing, keyboard shortcuts, drag/drop)
- Each instruction must be VERY exact and require only a
  SINGLE LEFT CLICK to accomplish
- Instructions must be concise: 1-15 words maximum
  (1 word if applicable)
- Be VERY specific in your instructions
- Reference specific UI elements and their purposes
- Provide actionable guidance that users can follow by
  left clicking
- Include both beginner and intermediate level left click
  actions
- Vary instruction structure: use diverse phrasings like
  "Open...", "Access...", "View...", "Save...",
  "Navigate to...", "Switch to...", "Add...", "Remove...",
  "Select...", etc.
- Avoid starting every instruction with "Click" - use
  natural action verbs that imply left clicking
- Focus on what's actually visible and left clickable in
  the interface
- Pay special attention to dynamic elements: open tabs,
  dropdowns, pop-ups, context menus, modal dialogs, and
  expandable sections if they exist
- Avoid overly verbose descriptions

Return your response as a JSON object with this exact
format:
{
  "interface_description": "Brief description of what
    type of interface this is and its main purpose",
  "segments": {
    "segment_name_1": {
      "description": "Brief description of this screen
        segment",
      "instructions": [
        "Specific instruction 1",
        "Specific instruction 2",
        ...
      ]
    },
    "segment_name_2": {
      "description": "Brief description of this screen
        segment",
      "instructions": [
        "Specific instruction 1",
        "Specific instruction 2",
        ...
      ]
    }
  }
}

DO NOT hypothesize features not visible in the image.
Only provide instructions based on what you can clearly
see.

REMEMBER: ALL instructions must be accomplished with a
SINGLE LEFT CLICK only. No right click, typing, keyboard
shortcuts, or multi-step actions.
Use diverse instruction phrasings - examples: "Open File
menu", "Save document", "View project structure",
"Access settings", "Navigate to homepage", "Switch tabs",
"Add new item", "Close dialog", "Expand dropdown",
"Select tab".
\end{lstlisting}
\end{figure}

\section{Instruction Relabeling Experiments}

\paragraph{Instruction Relabeling}
We want to understand how scaling the data impacts the SFT process. We trained Qwen 2.5 VL 7B on 10k samples with the hard samples filtered using GTA1 7B and easy samples filtered using Qwen 2.5 VL 7B. We also tried to improve the data by rewriting the prompts using LLM (Qwen3 4B) to remove noisy artifacts and by using image aware synthetic prompts (Qwen 2.5 VL 7B) to rewrite the prompts to include action intent.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
Rewriting Strategy & SS Pro & SS V2 \\
 & Accuracy & Accuracy \\
\hline
No Rewriting & 45.22\% & 91.05\% \\
Rewritten Prompts using LLM & 42.25\% & 88.84\% \\
(Qwen3 4B) & & \\
Image-aware Synthetic Prompts & 39.27\% & 85.86\% \\
(Qwen 2.5 VL 7B) & & \\
\hline
\end{tabular}
\caption{Impact of prompt rewriting strategies on downstream task performance.}
\label{tab:prompt_rewriting}
\end{table}

The following prompt template was used with Qwen3 4B to clean up noisy UI instructions:

\begin{figure}[h]
\begin{lstlisting}[
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{black!5},
  xleftmargin=1em,
  xrightmargin=1em,
  aboveskip=1em,
  belowskip=1em,
  caption={Prompt template used for instruction cleaning.},
  label={lst:cleaning_prompt}
]
You are a text editor that cleans up UI instructions.
Your task is to fix formatting, style, and noise issues
while preserving the exact intent and meaning.

Rules:
1. Fix grammatical errors and awkward phrasing
2. Remove overly verbose descriptions - keep instructions concise
3. Clean up technical noise (HTML tags, underscores, etc.)
4. NEVER change the core action or target element
5. Keep the output as a single, clear instruction

Examples:
Input:  "Choose Located in the top right corner."
Output: "Choose the element in the top right corner."

Input:  "Based on my descriptions, find the locations of
         the mentioned element in this webpage screenshot
         (with point). This element provides access to the
         main WhatsApp page, allowing users to navigate to
         the primary WhatsApp interface."
Output: "Find the element that provides access to the main
         WhatsApp page."

Input:  "click on new_tab"
Output: "Click on new tab."

Input:  "Click on the button labeled 'Submit' which is used
         to submit the form"
Output: "Click on the Submit button."

Now clean up this instruction:
{instruction}

Cleaned instruction:
\end{lstlisting}
\end{figure}

\section{Training Prompt Ablation}

Three system prompts were compared in this ablation. Their full text is shown below.

\begin{figure}[h]
\begin{lstlisting}[
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{black!5},
  xleftmargin=1em,
  xrightmargin=1em,
  aboveskip=1em,
  belowskip=1em,
  caption={Default Prompt (no resolution).},
  label={lst:prompt_default}
]
You are an expert UI element locator. Given a GUI
image and a user's element description, provide the
coordinates of the specified element as a single
(x,y) point. For elements with area, return the
center point.
Output the coordinate pair exactly:
(x,y)
\end{lstlisting}
\end{figure}

\begin{figure}[h]
\begin{lstlisting}[
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{black!5},
  xleftmargin=1em,
  xrightmargin=1em,
  aboveskip=1em,
  belowskip=1em,
  caption={Default Prompt + Image Resolution.},
  label={lst:prompt_default_res}
]
You are an expert UI element locator. Given a GUI
image and a user's element description, provide the
coordinates of the specified element as a single
(x,y) point. The image resolution is height {height}
and width {width}. For elements with area, return
the center point.
Output the coordinate pair exactly:
(x,y)
\end{lstlisting}
\end{figure}

\begin{figure}[h]
\begin{lstlisting}[
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{black!5},
  xleftmargin=1em,
  xrightmargin=1em,
  aboveskip=1em,
  belowskip=1em,
  caption={Qwen Tool Calling Prompt + Image Resolution (abbreviated).},
  label={lst:prompt_qwen_tool}
]
You are a helpful assistant.

# Tools
You may call one or more functions to assist with the
user query. You are provided with function signatures
within <tools></tools> XML tags:

<tools>
{"type": "function", "function":
  {"name": "computer_use",
   "description": "Use a mouse and keyboard to interact
     with a computer, and take screenshots.
     * This is an interface to a desktop GUI. You do not
       have access to a terminal or applications menu.
     * Some applications may take time to start or process
       actions, so you may need to wait and take successive
       screenshots to see the results of your actions.
     * The screen's resolution is {width}x{height}.
     * Whenever you intend to move the cursor to click on
       an element, consult a screenshot to determine the
       coordinates of the element before moving the cursor.
     * Make sure to click any buttons, links, icons, etc
       with the cursor tip in the center of the element.",
   "parameters": {"properties":
     {"action": {"description": "The action to perform.",
       "enum": ["key", "type", "mouse_move", "left_click",
         "left_click_drag", "right_click", "middle_click",
         "double_click", "scroll", "wait", "terminate"],
       "type": "string"}, ...},
     "required": ["action"], "type": "object"}}}
</tools>

For each function call, return a json object with
function name and arguments within <tool_call></tool_call>
XML tags:
<tool_call>{"name": <function-name>,
  "arguments": <args-json-object>}</tool_call>
\end{lstlisting}
\end{figure}

We trained Qwen 2.5 VL 7B on 10k samples with different training prompts to investigate two questions: (1) whether the base Qwen model's verbose tool-calling computer-use prompt is necessary for good performance, and (2) the impact of including image resolution in the prompt.

We find that the default prompt is sufficient for the model to perform well, achieving competitive results across benchmarks. The impact of including image resolution in the prompt is mixed---while it slightly improves performance on ScreenSpot V2, it actually decreases performance on ScreenSpot Pro.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
Prompt & SS Pro & SS V2 \\
 & Accuracy & Accuracy \\
\hline
Qwen Tool Calling Prompt + Image Resolution & 37.76\% & 88.59\% \\
Default Prompt + Image Resolution & 36.12\% & 88.59\% \\
Default Prompt & 39.03\% & 87.68\% \\
\hline
\end{tabular}
\caption{Impact of training prompt on downstream task performance for Qwen 2.5 VL 7B trained on 10k samples.}
\label{tab:prompt_ablation}
\end{table}

\section{Cold-Start SFT Budget}
\label{sec:coldstart}

Since \cite{qin2025uitars15} report a progressive training pipeline with continual SFT followed by RL, we conducted an experiment to understand the interaction between SFT and RL. We conducted cold-start experiments in which Qwen2.5-VL-7B-Instruct models are first fine-tuned on varying amounts of SFT data (1k, 3.3k, 10k, and 63k samples, each for a single epoch), and then trained with GRPO on the full 63k data pool for one epoch (63k was the size of our dataset at that time of the project). This experiment isolates how much the SFT initialization matters for subsequent RL gains.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/rl_coldstart_scaling_overlay.png}
\caption{Cold-start SFT budget experiment. Models fine-tuned on varying amounts of SFT data (1k, 3.3k, 10k, 63k) are subsequently trained with GRPO. RL performance scales monotonically with SFT data budget, confirming that RL benefits from a stronger initialization.}
\label{fig:rl_coldstart}
\end{figure}

As shown in Figure~\ref{fig:rl_coldstart}, RL performance scales monotonically with SFT data budget, confirming that RL benefits from a stronger initialization and motivating our choice of the 63k SFT model as the starting point for subsequent experiments.

\section{RL Hyperparameter Ablations}
\label{sec:rl_ablations}

We ablate key RL hyperparameters---sampling temperature, learning rate, and KL penalty weight---to establish robust defaults for all subsequent experiments.

\paragraph{Temperature.}
Using the 10k SFT model, we compare temperatures of 0.65, 0.85, and 1.0. Temperature controls the diversity of rollouts and thus the exploration--exploitation trade-off during RL training.

\paragraph{Learning Rate and KL Weight.}
We compare our default hyperparameters (LR = $1 \times 10^{-6}$, KL coef = $1 \times 10^{-2}$) with parameters inspired by UI-Venus~\cite{gu2025uivenus} (LR = $4 \times 10^{-7}$, KL coef = $4 \times 10^{-3}$) on the 63k SFT model. Our results show robustness to moderate variations in these hyperparameters.

\section{Hyperparameter Search and Model Configuration}

\subsection{Vision Tower Fine-tuning}
We investigated whether to freeze or fine-tune the vision tower during training. Results show that full fine-tuning significantly outperforms freezing the vision tower.

\begin{table}[h]
\centering
\begin{tabular}{lc}
\hline
Model Configuration & ScreenSpot Pro Accuracy \\
\hline
Full fine-tune & 45.22\% \\
Freeze vision tower & 32.95\% \\
\hline
\end{tabular}
\caption{Comparison of full fine-tuning versus freezing the vision tower for Qwen 2.5 VL 7B.}
\label{tab:vision_tower}
\end{table}

\subsection{Learning Rate Search}
We performed a learning rate sweep for Qwen 2.5 VL 7B on the 10k model-filtered dataset using Qwen2.5VL-7B for easy sample filtering and GTA1-7B for incorrect sample filtering.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
Learning Rate & ScreenSpot Pro & ScreenSpot V2 \\
 & Accuracy & Accuracy \\
\hline
1e-5 & 32.57\% & 80.02\% \\
5e-6 & 38.83\% & 87.80\% \\
\textbf{1e-6} & \textbf{45.35\%} & \textbf{90.27\%} \\
5e-7 & 38.07\% & 89.10\% \\
\hline
\end{tabular}
\caption{Learning rate sweep results. The optimal learning rate of 1e-6 (bold) achieves the best performance across all benchmarks.}
\label{tab:learning_rate_sweep}
\end{table}

