\chapter{Appendix}

\section{Supplemental Data}

This section contains supplemental information and data that supports the main thesis content.

\begin{table}[h]
\centering
\caption{Complete list of professional applications covered by the YouTube tutorial data collection pipeline.}
\label{tab:youtube_apps}
\begin{tabular}{llll}
\hline
3ds Max & Adobe Acrobat & Adobe After Effects & Adobe Dreamweaver \\
Adobe Illustrator & Adobe InDesign & Adobe Lightroom & Adobe Photoshop \\
Adobe Premiere & Airmail & Altium Designer & Android Studio \\
ANSYS & Apple Mail & Asana & Atom \\
AutoCAD & Autodesk Eagle & Autodesk Inventor & Autodesk Maya \\
Autodesk Revit & Avid Media Composer & Axure & Abaqus \\
Balsamiq & Blender & Brave & Burp Suite \\
Cadence Virtuoso & Catia & Cinema4D & COMSOL \\
Confluence & CryEngine & Cubase & DaVinci Resolve \\
DBeaver & Eviews & Figma & Final Cut Pro \\
FL Studio & Framer & Fusion 360 & GameMaker \\
GIMP & IBM SPSS & Intel Quartus Prime & IntelliJ IDEA \\
Jupyter & KiCad & LabVIEW & LibreOffice Base \\
LibreOffice Calc & LibreOffice Draw & LibreOffice Impress & LibreOffice Math \\
LibreOffice Writer & Logic Pro X & Looker & Mailbird \\
MATLAB & Microsoft Edge & ModelSim & Mozilla Firefox \\
Notion & OBS & Obsidian & OneNote \\
OriginLab & Outlook & Power BI & PyCharm \\
Quartus & RStudio & Simulink & SolidWorks \\
Stata & Tableau & Thunderbird & Unity \\
Unreal Engine & VLC Media Player & VMWare & Vivado \\
Xcode & & & \\
\hline
\end{tabular}
\end{table}

\section{Instruction Relabeling Experiments}

\paragraph{Instruction Relabeling}
We want to understand how scaling the data impacts the SFT process. We trained Qwen 2.5 VL 7B on 10k samples with the hard samples filtered using GTA1 7B and easy samples filtered using Qwen 2.5 VL 7B. We also tried to improve the data by rewriting the prompts using LLM (Qwen3 4B) to remove noisy artifacts and by using image aware synthetic prompts (Qwen 2.5 VL 7B) to rewrite the prompts to include action intent.

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\hline
Rewriting Strategy & SS Pro & SS V2 & Showdown Clicks & OS-World G \\
 & Accuracy & Accuracy & Accuracy & Accuracy \\
\hline
No Rewriting & 45.22\% & 91.05\% & 66.97\% & N/A \\
Rewritten Prompts using LLM & 42.25\% & 88.84\% & 66.24\% & N/A \\
(Qwen3 4B) & & & & \\
Image-aware Synthetic Prompts & 39.27\% & 85.86\% & 67.14\% & N/A \\
(Qwen 2.5 VL 7B) & & & & \\
\hline
\end{tabular}
\caption{Impact of prompt rewriting strategies on downstream task performance.}
\label{tab:prompt_rewriting}
\end{table}

The following prompt template was used with Qwen3 4B to clean up noisy UI instructions:

\begin{figure}[h]
\begin{lstlisting}[
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{black!5},
  xleftmargin=1em,
  xrightmargin=1em,
  aboveskip=1em,
  belowskip=1em,
  caption={Prompt template used for instruction cleaning.},
  label={lst:cleaning_prompt}
]
You are a text editor that cleans up UI instructions.
Your task is to fix formatting, style, and noise issues
while preserving the exact intent and meaning.

Rules:
1. Fix grammatical errors and awkward phrasing
2. Remove overly verbose descriptions - keep instructions concise
3. Clean up technical noise (HTML tags, underscores, etc.)
4. NEVER change the core action or target element
5. Keep the output as a single, clear instruction

Examples:
Input:  "Choose Located in the top right corner."
Output: "Choose the element in the top right corner."

Input:  "Based on my descriptions, find the locations of
         the mentioned element in this webpage screenshot
         (with point). This element provides access to the
         main WhatsApp page, allowing users to navigate to
         the primary WhatsApp interface."
Output: "Find the element that provides access to the main
         WhatsApp page."

Input:  "click on new_tab"
Output: "Click on new tab."

Input:  "Click on the button labeled 'Submit' which is used
         to submit the form"
Output: "Click on the Submit button."

Now clean up this instruction:
{instruction}

Cleaned instruction:
\end{lstlisting}
\end{figure}

\section{Training Prompt Ablation}

Three system prompts were compared in this ablation. Their full text is shown below.

\begin{figure}[h]
\begin{lstlisting}[
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{black!5},
  xleftmargin=1em,
  xrightmargin=1em,
  aboveskip=1em,
  belowskip=1em,
  caption={Default Prompt (no resolution).},
  label={lst:prompt_default}
]
You are an expert UI element locator. Given a GUI
image and a user's element description, provide the
coordinates of the specified element as a single
(x,y) point. For elements with area, return the
center point.
Output the coordinate pair exactly:
(x,y)
\end{lstlisting}
\end{figure}

\begin{figure}[h]
\begin{lstlisting}[
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{black!5},
  xleftmargin=1em,
  xrightmargin=1em,
  aboveskip=1em,
  belowskip=1em,
  caption={Default Prompt + Image Resolution.},
  label={lst:prompt_default_res}
]
You are an expert UI element locator. Given a GUI
image and a user's element description, provide the
coordinates of the specified element as a single
(x,y) point. The image resolution is height {height}
and width {width}. For elements with area, return
the center point.
Output the coordinate pair exactly:
(x,y)
\end{lstlisting}
\end{figure}

\begin{figure}[h]
\begin{lstlisting}[
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{black!5},
  xleftmargin=1em,
  xrightmargin=1em,
  aboveskip=1em,
  belowskip=1em,
  caption={Qwen Tool Calling Prompt + Image Resolution (abbreviated).},
  label={lst:prompt_qwen_tool}
]
You are a helpful assistant.

# Tools
You may call one or more functions to assist with the
user query. You are provided with function signatures
within <tools></tools> XML tags:

<tools>
{"type": "function", "function":
  {"name": "computer_use",
   "description": "Use a mouse and keyboard to interact
     with a computer, and take screenshots.
     * This is an interface to a desktop GUI. You do not
       have access to a terminal or applications menu.
     * Some applications may take time to start or process
       actions, so you may need to wait and take successive
       screenshots to see the results of your actions.
     * The screen's resolution is {width}x{height}.
     * Whenever you intend to move the cursor to click on
       an element, consult a screenshot to determine the
       coordinates of the element before moving the cursor.
     * Make sure to click any buttons, links, icons, etc
       with the cursor tip in the center of the element.",
   "parameters": {"properties":
     {"action": {"description": "The action to perform.",
       "enum": ["key", "type", "mouse_move", "left_click",
         "left_click_drag", "right_click", "middle_click",
         "double_click", "scroll", "wait", "terminate"],
       "type": "string"}, ...},
     "required": ["action"], "type": "object"}}}
</tools>

For each function call, return a json object with
function name and arguments within <tool_call></tool_call>
XML tags:
<tool_call>{"name": <function-name>,
  "arguments": <args-json-object>}</tool_call>
\end{lstlisting}
\end{figure}

We trained Qwen 2.5 VL 7B on 10k samples with different training prompts to investigate two questions: (1) whether the base Qwen model's verbose tool-calling computer-use prompt is necessary for good performance, and (2) the impact of including image resolution in the prompt.

We find that the default prompt is sufficient for the model to perform well, achieving competitive results across benchmarks. The impact of including image resolution in the prompt is mixed---while it slightly improves performance on ScreenSpot V2 and Showdown Clicks, it actually decreases performance on ScreenSpot Pro.

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\hline
Prompt & SS Pro & SS V2 & Showdown Clicks & OS-World G \\
 & Accuracy & Accuracy & Accuracy & Accuracy \\
\hline
Qwen Tool Calling Prompt + Image Resolution & 37.76\% & 88.59\% & 61.94\% & N/A \\
Default Prompt + Image Resolution & 36.12\% & 88.59\% & 66.43\% & N/A \\
Default Prompt & 39.03\% & 87.68\% & 64.09\% & N/A \\
\hline
\end{tabular}
\caption{Impact of training prompt on downstream task performance for Qwen 2.5 VL 7B trained on 10k samples.}
\label{tab:prompt_ablation}
\end{table}

\section{Hyperparameter Search and Model Configuration}

\subsection{Vision Tower Fine-tuning}
We investigated whether to freeze or fine-tune the vision tower during training. Results show that full fine-tuning significantly outperforms freezing the vision tower.

\begin{table}[h]
\centering
\begin{tabular}{lc}
\hline
Model Configuration & ScreenSpot Pro Accuracy \\
\hline
Full fine-tune & 45.22\% \\
Freeze vision tower & 32.95\% \\
\hline
\end{tabular}
\caption{Comparison of full fine-tuning versus freezing the vision tower for Qwen 2.5 VL 7B.}
\label{tab:vision_tower}
\end{table}

\subsection{Learning Rate Search}
We performed a learning rate sweep for Qwen 2.5 VL 7B on the 10k model-filtered dataset using Qwen2.5VL-7B for easy sample filtering and GTA1-7B for incorrect sample filtering.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\hline
Learning Rate & ScreenSpot Pro & ScreenSpot V2 & Showdown Clicks \\
 & Accuracy & Accuracy & Accuracy \\
\hline
1e-5 & 32.57\% & 80.02\% & 53.68\% \\
5e-6 & 38.83\% & 87.80\% & 63.01\% \\
\textbf{1e-6} & \textbf{45.35\%} & \textbf{90.27\%} & \textbf{67.50\%} \\
5e-7 & 38.07\% & 89.10\% & 65.17\% \\
\hline
\end{tabular}
\caption{Learning rate sweep results. The optimal learning rate of 1e-6 (bold) achieves the best performance across all benchmarks.}
\label{tab:learning_rate_sweep}
\end{table}

